{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9BfQz3bBZumki3DBLZuO/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjoOq5L0JOcJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import glob\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Download and extract the zip file\n",
        "!curl -LO https://github.com/MrTimonM/cap-dataset/releases/download/v1.10/train.zip\n",
        "!unzip -qq train.zip\n",
        "\n",
        "# Define the directory where the extracted images are located\n",
        "extracted_dir = 'train'\n",
        "\n",
        "# Loop through all the images in the extracted directory\n",
        "for filename in os.listdir(extracted_dir):\n",
        "    filepath = os.path.join(extracted_dir, filename)\n",
        "    # Open the image\n",
        "    with Image.open(filepath) as img:\n",
        "        # Resize the image to the desired dimensions\n",
        "        img_resized = img.resize((270, 130))\n",
        "        # Save the resized image, overwriting the original\n",
        "        img_resized.save(filepath)\n",
        "\n",
        "print(\"All images resized successfully.\")\n"
      ],
      "metadata": {
        "id": "sMGmCgm7JaO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = 'train'\n",
        "H, W, C = 130, 270, 3  # height, width, 3(RGB channels)\n",
        "label_choices = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "# N_LABELS = len(label_choices)  # label_size\n",
        "N_LABELS = 256\n",
        "D = 4 # num_per_image"
      ],
      "metadata": {
        "id": "2WoD9JvjKcO7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_filepath(filepath):\n",
        "    try:\n",
        "        path, filename = os.path.split(filepath)\n",
        "        filename, ext = os.path.splitext(filename)\n",
        "        label, _ = filename.split(\"_\")\n",
        "        return label\n",
        "    except Exception as e:\n",
        "        print('error to parse %s. %s' % (filepath, e))\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "LtQXXtizKcRf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob2\n",
        "import pandas as pd\n",
        "\n",
        "# Function to parse file path and extract attributes\n",
        "def parse_filepath(filepath):\n",
        "    filename = os.path.basename(filepath)\n",
        "    label = filename.split('.')[0]  # Assuming label is the part before the first '.'\n",
        "    return label, filepath\n",
        "\n",
        "# Create a pandas DataFrame of images, age, gender, and race\n",
        "DATA_DIR = 'train'  # Update this with your data directory path\n",
        "files = glob2.glob(os.path.join(DATA_DIR, \"*.png\"))\n",
        "\n",
        "if not files:\n",
        "    raise ValueError(\"No image files found in the specified directory.\")\n",
        "\n",
        "attributes = [parse_filepath(file) for file in files]\n",
        "\n",
        "# Check if attributes have consistent length\n",
        "if attributes:\n",
        "    num_attributes = len(attributes[0])\n",
        "    for attr in attributes:\n",
        "        if len(attr) != num_attributes:\n",
        "            raise ValueError(\"Attributes have inconsistent length.\")\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(attributes, columns=['label', 'file'])\n",
        "\n",
        "    # Drop any rows with NaN values\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Display the DataFrame\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(\"No files found.\")\n"
      ],
      "metadata": {
        "id": "Qk-MIYAvKcUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # Add this line to import NumPy\n",
        "\n",
        "p = np.random.permutation(len(df))\n",
        "train_up_to = int(len(df) * 0.7)\n",
        "train_idx = p[:train_up_to]\n",
        "test_idx = p[train_up_to:]\n",
        "\n",
        "# split train_idx further into training and validation set\n",
        "train_up_to = int(train_up_to * 0.7)\n",
        "train_idx, valid_idx = train_idx[:train_up_to], train_idx[train_up_to:]\n",
        "\n",
        "print('train count: %s, valid count: %s, test count: %s' % (\n",
        "    len(train_idx), len(valid_idx), len(test_idx)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuiNlCxSLUsb",
        "outputId": "161d4aee-afd1-41c9-d582-c4e54796c105"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train count: 970, valid count: 417, test count: 595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def get_data_generator(df, indices, for_training, batch_size=16):\n",
        "    images, labels = [], []\n",
        "    while True:\n",
        "        for i in indices:\n",
        "            r = df.iloc[i]\n",
        "            file, label = r['file'], r['label']\n",
        "            im = Image.open(file)\n",
        "#             im = im.resize((H, W))\n",
        "            im = np.array(im) / 255.0\n",
        "            images.append(np.array(im))\n",
        "            labels.append(np.array([np.array(to_categorical(ord(i), N_LABELS)) for i in label]))\n",
        "            if len(images) >= batch_size:\n",
        "#                 print(np.array(images), np.array(labels))\n",
        "                yield np.array(images), np.array(labels)\n",
        "                images, labels = [], []\n",
        "        if not for_training:\n",
        "            break"
      ],
      "metadata": {
        "id": "tM-B-LHBLi4-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf  # Add this line to import TensorFlow\n",
        "from tensorflow.keras import layers, models  # Add this line to import layers and models from TensorFlow.keras\n",
        "\n",
        "input_layer = tf.keras.Input(shape=(H, W, C))\n",
        "x = layers.Conv2D(32, 3, activation='relu')(input_layer)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "\n",
        "x = layers.Dense(D * N_LABELS, activation='softmax')(x)\n",
        "x = layers.Reshape((D, N_LABELS))(x)\n",
        "\n",
        "model = models.Model(inputs=input_layer, outputs=x)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "YW-9m0nlLi8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "batch_size = 64\n",
        "valid_batch_size = 64\n",
        "train_gen = get_data_generator(df, train_idx, for_training=True, batch_size=batch_size)\n",
        "valid_gen = get_data_generator(df, valid_idx, for_training=True, batch_size=valid_batch_size)\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\"./model_checkpoint\", monitor='val_loss')\n",
        "]\n",
        "\n",
        "history = model.fit(train_gen,\n",
        "                    steps_per_epoch=len(train_idx)//batch_size,\n",
        "                    epochs=3,\n",
        "#                     callbacks=callbacks,\n",
        "                    validation_data=valid_gen,\n",
        "                    validation_steps=len(valid_idx)//valid_batch_size)"
      ],
      "metadata": {
        "id": "I_ovDUD9L3GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_train_history(history):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
        "\n",
        "    axes[0].plot(history.history['accuracy'], label='Train accuracy')\n",
        "    axes[0].plot(history.history['val_accuracy'], label='Val accuracy')\n",
        "    axes[0].set_xlabel('Epochs')\n",
        "    axes[0].legend()\n",
        "\n",
        "    axes[1].plot(history.history['loss'], label='Training loss')\n",
        "    axes[1].plot(history.history['val_loss'], label='Validation loss')\n",
        "    axes[1].set_xlabel('Epochs')\n",
        "    axes[1].legend()\n",
        "\n",
        "plot_train_history(history)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v-ErYDQVNLVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate loss and accuracy in test dataset\n",
        "test_gen = get_data_generator(df, test_idx, for_training=False, batch_size=128)\n",
        "dict(zip(model.metrics_names, model.evaluate(test_gen, steps=len(test_idx)//128)))"
      ],
      "metadata": {
        "id": "XjP-fggBNX2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir('./saved_model'): os.mkdir('./saved_model')\n",
        "model.save('saved_model/alexnet-4char-with-upper-letters')"
      ],
      "metadata": {
        "id": "grLx2Et4Nvg_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_gen = get_data_generator(df, test_idx, for_training=False, batch_size=128)\n",
        "x_test, y_test = next(test_gen)\n",
        "\n",
        "y_pred = model.predict_on_batch(x_test)\n",
        "\n",
        "y_true = tf.math.argmax(y_test, axis=-1)\n",
        "y_pred = tf.math.argmax(y_pred, axis=-1)"
      ],
      "metadata": {
        "id": "Nh1M-vHJN1hI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_y(y):\n",
        "    return ''.join(map(lambda x: chr(int(x)), y))"
      ],
      "metadata": {
        "id": "vrJ4fA2lN4g8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "n = 30\n",
        "random_indices = np.random.permutation(n)\n",
        "n_cols = 5\n",
        "n_rows = math.ceil(n / n_cols)\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 20))\n",
        "for i, img_idx in enumerate(random_indices):\n",
        "    ax = axes.flat[i]\n",
        "    ax.imshow(x_test[img_idx])\n",
        "    ax.set_title('pred: %s' % format_y(y_pred[img_idx]))\n",
        "    ax.set_xlabel('true: %s' % format_y(y_true[img_idx]))\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N4EWTF-ON4nL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}